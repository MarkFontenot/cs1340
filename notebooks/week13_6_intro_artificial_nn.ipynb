{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are parallel computing devices, which is basically an attempt to make a computer model of the brain. The main objective is to develop a system to perform various computational tasks faster than the traditional systems. These tasks include pattern recognition and classification, approximation, optimization, and data clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Artificial Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network ANN\n",
    "\n",
    "is an efficient computing system whose central theme is borrowed from the analogy of biological neural networks. ANNs are also named as ‚Äúartificial neural systems,‚Äù or ‚Äúparallel distributed processing systems,‚Äù or ‚Äúconnectionist systems.‚Äù ANN acquires a large collection of units that are interconnected in some pattern to allow communication between the units. These units, also referred to as nodes or neurons, are simple processors which operate in parallel.\n",
    "\n",
    "Every neuron is connected with other neuron through a connection link. Each connection link is associated with a weight that has information about the input signal. This is the most useful information for neurons to solve a particular problem because the weight usually excites or inhibits the signal that is being communicated. Each neuron has an internal state, which is called an activation signal. Output signals, which are produced after combining the input signals and activation rule, may be sent to other units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief History of ANN\n",
    "\n",
    "- 1940s - The beginning of Neural Networks (Electronic Brain)\n",
    "- 1950s and 1960s - The first golden age of Neural Networks (Perceptron)\n",
    "- 1970s - The winter of Neural Networks (XOR problem)\n",
    "- 1980s - Renewed enthusiasm (Multilayered Perceptron, backpropagation)\n",
    "- 1990s - Subfield of Radial Basis Function Networks was developed\n",
    "- 2000s - The power of Neural Networks Ensembles & Support Vector Machines is apparent\n",
    "- 2006 - Hinton presents the Deep Belief Network (DBN)\n",
    "- 2009 - Deep Recurrent Neural Network\n",
    "- 2010 - Convolutional Deep Belief Network (CDBN)\n",
    "- 2011 - Max-Pooling CDBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/ann_history.jpg' width=\"800\">\n",
    "\n",
    "[image source] http://qingkaikong.blogspot.com/2016/11/machine-learning-3-artificial-neural.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neurons: An Overly Simplified Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/neuron_1.png' width=\"500\">\n",
    "\n",
    "[image source] https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dendrite**: Receives signals from other neurons\n",
    "\n",
    "- **Soma**: Processes the information\n",
    "\n",
    "- **Axon**: Transmits the output of this neuron\n",
    "\n",
    "- **Synapse**: Point of connection to other neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, a neuron takes an input signal (dendrite), processes it like the CPU (soma), passes the output through a cable like structure to other connected neurons (axon to synapse to other neuron‚Äôs dendrite). Now, this might be biologically inaccurate as there is a lot more going on out there but on a higher level, this is what is going on with a neuron in our brain ‚Äî takes an input, processes it, throws out an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sense organs interact with the outer world and send the visual and sound information to the neurons. Let's say you are watching Friends. Now the information your brain receives is taken in by the ‚Äúlaugh or not‚Äù set of neurons that will help you make a decision on whether to laugh or not. Each neuron gets fired/activated only when its respective criteria (more on this later) is met like shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/neuron_2.png' width=\"1000\">\n",
    "\n",
    "[image source] https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch-Pitts Neuron\n",
    "\n",
    "The first computational model of a neuron was proposed by Warren MuCulloch (neuroscientist) and Walter Pitts (logician) in 1943."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/m_pitts_neuron.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple  Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/perceptron.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 things are happening here. First, each input is multiplied by a weight: \n",
    "\n",
    "$x_1 -> x_1 * w_1$\n",
    "\n",
    "$x_2 -> x_2 * w_2$\n",
    "\n",
    "Next, all the weighted inputs are added together with a bias b:\n",
    "\n",
    "$(x_1*w_1) + (x_2*w_2) + b$\n",
    "\n",
    "Finally, the sum is passed through an activation function: \n",
    "\n",
    "$y = f(x_1*w1 + x_2*w_2 + b)$\n",
    "\n",
    "The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/sigmoid.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (‚àí‚àû,+‚àû)to (0,1) - big negative numbers become ~0, and big positive numbers become ~1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a 2-input neuron that uses the sigmoid activation function and has the following parameters:\n",
    "\n",
    "$w = [0, 1]$\n",
    "\n",
    "$b = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = [0, 1]$ is just a way of writing w1=0, w2=1 in vector form. Now, let‚Äôs give the neuron an input of x=[2,3]. We‚Äôll use the dot product to write things more concisely:\n",
    "\n",
    "\n",
    "$(w*x) + b  = ((w_1* x_1) + (w_2*x_2)) + b $\n",
    "\n",
    "            = 0 * 2 + 1 * 3 + 4 \n",
    "           \n",
    "            = 7\n",
    "            \n",
    "$y = f(w*x + b) = f(7) = 0.999$\n",
    "\n",
    "The neuron outputs 0.999 given the inputs x=[2,3]. \n",
    "\n",
    "That‚Äôs it! This process of passing inputs forward to get an output is known as **feedforward**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to implement a neuron! We‚Äôll use NumPy, a popular and powerful computing library for Python, to help us do math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        # Weight inputs, add bias, then use the activation function\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognize those numbers? That‚Äôs the example we just did! We get the same answer of 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Neurons into a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is nothing more than a bunch of neurons connected together. Here‚Äôs what a simple neural network might look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/network.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network has 2 inputs, a hidden layer with 2 neurons (h1 and h2), and an output layer with 1 neuron (o1). Notice that the inputs for o1 are the outputs from h1 and h2 - that‚Äôs what makes this a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **hidden layer** is any layer between the input (first) layer and output (last) layer. **There can be multiple hidden layers!** \n",
    "\n",
    "The new coined concept **Deep Learning** is just a neural network with tens of or even hundreds of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example: Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs use the network pictured above and assume all neurons have the same weights w=[0,1], the same bias b=0, and the same sigmoid activation function. Let h1,h2,o1 denote the outputs of the neurons they represent.\n",
    "\n",
    "What happens if we pass in the input x=[2,3]?\n",
    "\n",
    "$h1 = h2 = f(w*x + b)$\n",
    "\n",
    "    = f((0*2) + (1*3) + 0)\n",
    "    \n",
    "    = f(3)\n",
    "    \n",
    "    = 0.9526\n",
    "    \n",
    "$o1 = f(w[h1, h2] + b)$\n",
    "\n",
    "    = f((0*h1) + (1*h1) + 0)\n",
    "    \n",
    "    = f(0.9526)\n",
    "    \n",
    "    = 0.7216\n",
    "    \n",
    "    \n",
    "The output of the neural network for input x=[2,3] is 0.7216. Pretty simple, right?\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can have any number of layers with any number of neurons in those layers. The basic idea stays the same: feed the input(s) forward through the neurons in the network to get the output(s) at the end. For simplicity, we‚Äôll keep using the network pictured above for the rest of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a Neural Network: Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "    Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "\n",
    "        # The Neuron class here is from the previous section\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "        # The inputs for o1 are the outputs from h1 and h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 0.7216 again! Looks like it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/gender.png' width=\"800\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs train our network to predict someone‚Äôs gender given their weight and height:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/network2.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôll represent Male with a 0 and Female with a 1, and we‚Äôll also shift the data to make it easier to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/table2.png' width=\"800\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I arbitrarily chose the shift amounts (135 and 66) to make the numbers look nice. Normally, you‚Äôd shift by the mean.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our network, we first need a way to quantify how ‚Äúgood‚Äù it‚Äôs doing so that it can try to do ‚Äúbetter‚Äù. That‚Äôs what the loss is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôll use the **mean squared error (MSE)** loss:\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_{true} - y_{pred})^2$\n",
    "\n",
    "Let‚Äôs break this down:\n",
    "\n",
    "- n is the number of samples, which is 4 (Alice, Bob, Charlie, Diana).\n",
    "- y represents the variable being predicted, which is Gender.\n",
    "- $y_{true}$ is the true value of the variable (the ‚Äúcorrect answer‚Äù). For example, $y_{true}$ for Alice would be  (Female).\n",
    "\n",
    "- $y_{pred}$ is the predicted value of the variable. It‚Äôs whatever our network outputs.\n",
    "\n",
    "\n",
    "$(y_{true} - y_{pred})^2$ is known as the **squared error**. Our loss function is simply taking the average over all squared errors (hence the name mean squared error). The better our predictions are, the lower our loss will be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better predictions = Lower loss.\n",
    "\n",
    "**Training a network = trying to minimize its loss. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example Loss Calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs say our network always outputs 0 - in other words, it‚Äôs confident all humans are Male ü§î. What would our loss be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/table3.png' width=\"800\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE = \\frac{1}{4}(1 + 0 + 0 + 1) = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: MSE Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs some code to calculate loss for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network Part 2  (Math!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have a clear goal: minimize the loss of the neural network**. We know we can change the network‚Äôs weights and biases to influence its predictions, but how do we do so in a way that decreases loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let‚Äôs pretend we only have Alice in our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/table_alice.png' width=\"800\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the mean squared error loss is just Alice‚Äôs squared error:\n",
    "\n",
    "$MSE = \\frac{1}{1}\\sum_{i=1}^1(y_{true} - y_{pred})$\n",
    "\n",
    "$     = (y_{true} - y_{pred})^2$\n",
    "\n",
    "$     = (1 - y_{pred})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs label each weight and bias in our network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/network3.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write loss as a multivariable function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lectures, we mentioned use **gradient descent** to calculate the partial derivatives.  \n",
    "\n",
    "more info : https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.336\n",
      "Epoch 10 loss: 0.208\n",
      "Epoch 20 loss: 0.132\n",
      "Epoch 30 loss: 0.090\n",
      "Epoch 40 loss: 0.066\n",
      "Epoch 50 loss: 0.051\n",
      "Epoch 60 loss: 0.041\n",
      "Epoch 70 loss: 0.034\n",
      "Epoch 80 loss: 0.028\n",
      "Epoch 90 loss: 0.025\n",
      "Epoch 100 loss: 0.022\n",
      "Epoch 110 loss: 0.019\n",
      "Epoch 120 loss: 0.017\n",
      "Epoch 130 loss: 0.016\n",
      "Epoch 140 loss: 0.014\n",
      "Epoch 150 loss: 0.013\n",
      "Epoch 160 loss: 0.012\n",
      "Epoch 170 loss: 0.011\n",
      "Epoch 180 loss: 0.011\n",
      "Epoch 190 loss: 0.010\n",
      "Epoch 200 loss: 0.009\n",
      "Epoch 210 loss: 0.009\n",
      "Epoch 220 loss: 0.008\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 240 loss: 0.007\n",
      "Epoch 250 loss: 0.007\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 280 loss: 0.006\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 320 loss: 0.005\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 380 loss: 0.004\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 470 loss: 0.003\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 630 loss: 0.002\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "    *** DISCLAIMER ***:\n",
    "    The code below is intended to be simple and educational, NOT optimal.\n",
    "    Real neural net code looks nothing like this. DO NOT use this code.\n",
    "    Instead, read/run it to understand how this specific network works.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "        - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "        - all_y_trues is a numpy array with n elements.\n",
    "          Elements in all_y_trues correspond to those in data.\n",
    "        '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Calculate partial derivatives.\n",
    "                # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/loss.png' width=\"500\">\n",
    "[image source] https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.964\n",
      "Frank: 0.039\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensoflow/keras (Google)\n",
    "# Pytorch   (Facebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
